UTF-16 (англ. Unicode Transformation Format) в информатике — один из способов кодирования символов из Юникода в виде последовательности 16-битных слов. Данная кодировка позволяет записывать символы Юникода в диапазонах U+0000..U+D7FF и U+E000..U+10FFFF (общим количеством 1 112 064). При этом каждый символ записывается одним или двумя словами (суррогатная пара).
Теперь, когда Юникод содержит больше, чем 65536 символов, он не может вместить их все в 2 байта. Это значит, что один экземпляр структуры Char не может принимать все возможные символы. UTF-16 (и .NET) решает эту проблему путём использования суррогатных пар (surrogate pair) — это два 16-битных значения, где каждое значение лежит в диапазоне от 0xD800 и до 0xDFFF.
Суррогатные пары делятся на две части — «верхнюю» (D800–DBFF) и [BLOCK:low-surrogates «нижнюю»] (DC00–DFFF).